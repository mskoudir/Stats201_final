# -*- coding: utf-8 -*-
"""stats201_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gaupAFyoWRsOOjnlQgZVJ0dP1tT7E-wp
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler

# ---------------------
# Load the dataset
# ---------------------
data = pd.read_csv('./ecommerce_customer_behavior.csv')
print("Dataset shape:", data.shape)
print("Dataset preview:")
print(data.head())

# ---------------------
# Data Exploration
# ---------------------
print("\nMissing values per column:")
print(data.isnull().sum())
print("\nData types:")
print(data.dtypes)

# There are 2 missing values in 'Satisfaction Level'. Drop these rows.
data = data.dropna(subset=['Satisfaction Level'])

# ---------------------
# Create Target Variable
# ---------------------
# We'll define a binary target: 'Satisfied' if Satisfaction Level is "Satisfied", else "Not Satisfied"
data['Satisfied'] = data['Satisfaction Level'].apply(lambda x: 1 if x.strip().lower() == "satisfied" else 0)
print("\nTarget variable distribution:")
print(data['Satisfied'].value_counts())

# ---------------------
# Data Preprocessing
# ---------------------
# We will drop columns that are not needed for modeling: "Customer ID" and "Satisfaction Level"
data_model = data.drop(columns=["Customer ID", "Satisfaction Level"])

# Define categorical features and convert them using LabelEncoder
categorical_features = ['Gender', 'City', 'Membership Type']
le = LabelEncoder()
for col in categorical_features:
    data_model[col] = le.fit_transform(data_model[col])

# Display updated dataframe head
print("\nData after encoding categorical variables:")
print(data_model.head())

# Define feature set (X) and target (y)
# Features: Age, Total Spend, Items Purchased, Average Rating, Discount Applied (bool as 0/1),
# Days Since Last Purchase, plus encoded categorical features.
feature_columns = ['Age', 'Total Spend', 'Items Purchased', 'Average Rating',
                   'Discount Applied', 'Days Since Last Purchase'] + categorical_features

X = data_model[feature_columns]
y = data_model['Satisfied']

# Ensure boolean column is converted to numeric (True->1, False->0)
X['Discount Applied'] = X['Discount Applied'].astype(int)

# Feature Scaling for numeric features
numeric_features = ['Age', 'Total Spend', 'Items Purchased', 'Average Rating', 'Days Since Last Purchase']
scaler = StandardScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# ---------------------
# Split Data into Train and Test Sets
# ---------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
print("\nTraining set size:", X_train.shape)
print("Test set size:", X_test.shape)

# ---------------------
# Model Building: Logistic Regression with Cross-Validation
# ---------------------
model = LogisticRegression(max_iter=1000, solver='liblinear')
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
print(f"\nCross-Validation Accuracy Scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean()*100:.2f}%")

# Fit the model on the training data
model.fit(X_train, y_train)

# ---------------------
# Model Evaluation
# ---------------------
# Predict on test set
y_pred = model.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"\nTest Accuracy: {test_accuracy*100:.2f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix Visualization
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
y_prob = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", lw=2)
plt.plot([0, 1], [0, 1], "k--", label="Random Chance")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

# ---------------------
# Feature Importance (Logistic Regression Coefficients)
# ---------------------
coefficients = pd.Series(model.coef_[0], index=feature_columns).sort_values()
plt.figure(figsize=(8, 6))
coefficients.plot(kind="barh")
plt.xlabel("Coefficient Value")
plt.title("Feature Importance from Logistic Regression")
plt.show()

# Output model coefficients for interpretation
print("\nModel Coefficients:")
print(coefficients)
